To address the common problem of high dimensionality in tensor regressions, we introduce a generalized tensor random projection method that embeds high-dimensional tensor-valued covariates into low-dimensional subspaces with little loss of information about the response. The method is flexible, allowing for tensor-wise, mode-wise, or combined random projections as special cases. A Bayesian inference framework is provided, featuring the use of a hierarchical prior distribution and a low-rank representation of the parameter. Strong theoretical support is provided for the concentration properties of the random projection and posterior consistency of the Bayesian inference. An efficient Gibbs sampler is developed to perform inference on the compressed data. To mitigate the sensitivity introduced by random projections, Bayesian model averaging is employed, with normalizing constants estimated using reverse logistic regression. An extensive simulation study is conducted to examine the effects of different tuning parameters. Simulations indicate, and the real data application confirms, that compressed Bayesian tensor regressions can achieve better out-of-sample predictions while significantly reducing computational costs compared to standard Bayesian tensor regressions.

To regenerate the scatter plots in Figure 3 of the paper:
1. Run the script cbtr_sim_run.py to generate output files with options to choose different settings ('diagnol', 'circle', 'swiss', 'L-shape', 'stripes', 'blocks', 'random') and type of random projection (mode-wise, tensor-wise).
2. Run the script cbtr_sim_post.py to post processing the output from cbtr_sim_run.py and plot the figures.
